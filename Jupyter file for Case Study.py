# -*- coding: utf-8 -*-
"""Blackcoffer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/183QYTuDOgyLOqKlwOD1QSIKD55VDbrOT
"""

from google.colab import drive
drive.mount('/content/drive')

import requests
from bs4 import BeautifulSoup
import glob
import pandas as pd
import numpy as np
import string
import re
import nltk
nltk.download('punkt') # Download NLTK resources
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords') # Download the NLTK stop words
from nltk.tokenize import sent_tokenize
!pip install textstat
from textstat import textstat

# Read the input Excel file
input_file = pd.read_excel(r"/content/drive/MyDrive/BlackCoffer Data Engineer Assignment/Provided files/Input.xlsx")

# Extract the title and text of article using the function
def extract_data(url):
    try:
        request = requests.get(url)
        soup = BeautifulSoup(request.text, "html.parser")
        title = soup.title.get_text()
        article_text = ""
        # check if the article content is in the class 'td-post-content'
        if soup.find("div", class_="td-post-content"):
           article_text = soup.find("div", class_="td-post-content").text.strip()
        # Check if the article content is in the class 'tdb-block-inner
        elif soup.find("div", class_="tdb-block-inner"):
            article_text = soup.find("div", class_="tdb-block-inner").text.strip()
        return title,article_text
    except Exception as e:
        print(f"An error occurred with URL {url}: {e}")
        return None, None
# Iterate over each row in the DataFrame
for index,row in input_file.iterrows():
    url_id = row['URL_ID']
    url = row['URL']
    # Extract data from the specified URL
    title, text = extract_data(url)
    # Save the extracted article text in a text file
    if text:
        with open(f'{url_id}.txt', 'w', encoding='utf-8') as file:
            file.write(f'Title: {title}\n\n')
            file.write(f'Text: {text}\n')
        print(f"Data extracted and saved for URL_ID: {url_id}")
    else:
        print(f"Data extraction failed for URL_ID: {url_id}")

print("Data extraction complete.")

import os

# Define the path to the specific folder in Google Drive where you want to save the files
folder_path = '/content/drive/My Drive/BlackCoffer Data Engineer Assignment/Extracted text files/'

# Make sure the folder exists, if not, create it
if not os.path.exists(folder_path):
    os.makedirs(folder_path)

# Iterate over each row in the DataFrame
for index, row in input_file.iterrows():
    url_id = row['URL_ID']
    url = row['URL']
    # Extract data from the specified URL
    title, text = extract_data(url)
    # Save the extracted article text in a text file within the specified folder
    if text:
        file_path = os.path.join(folder_path, f'{url_id}.txt')
        with open(file_path, 'w', encoding='utf-8') as file:
            file.write(f'Title: {title}\n\n')
            file.write(f'Text: {text}\n')
        print(f"Data extracted and saved for URL_ID: {url_id} in folder: {folder_path}")
    else:
        print(f"Data extraction failed for URL_ID: {url_id}")

print("Data extraction complete.")

text_data=[]
# Iterate over each text file
for file in glob.glob('*.txt'):
    with open(file, 'r', encoding='latin-1') as files:
        title = ""
        text = ""
        # Flag to indicate if we are reading text content
        is_text = False
        # Counter for consecutive empty lines
        empty_line_count = 0
        for line in files:
            if line.strip().startswith("Title:"):
                title = line.strip().split('Title: ')[1]
            elif line.strip().startswith("Text:"):
                text = line.strip().split('Text: ')[1]
                is_text = True
            # Check if we are in the text section and the line is not empty
            elif is_text and line.strip():
                # Append the line to the text
                text += " " + line.strip()
                # Reset the empty line counter
                empty_line_count = 0
            # Check if we are in the text section and the line is empty
            elif is_text and not line.strip():
                # Increment the empty line counter
                empty_line_count += 1
                # If two consecutive lines are empty, stop reading text
                if empty_line_count == 2:
                    break
        # Extract the url_id from the file name
        url_id = file.split('.')[0]
        text_data.append({'URL_ID': url_id,'Title': title, 'Text': text})

# Create a DataFrame from the extracted data
extracted_df = pd.DataFrame(text_data)
print(extracted_df)

extracted_df.head()

text_data = []
# Iterate over each text file
for file in glob.glob('*.txt'):
    with open(file, 'r', encoding='latin-1') as files:
        title = ""
        text = ""
        # Flag to indicate if we are reading text content
        is_text = False
        # Counter for consecutive empty lines
        empty_line_count = 0
        for line in files:
            if line.strip().startswith("Title:"):
                title = line.strip().split('Title: ')[1]
            elif line.strip().startswith("Text:"):
                text = line.strip().split('Text: ')[1]
                is_text = True
            # Check if we are in the text section and the line is not empty
            elif is_text and line.strip():
                # Append the line to the text
                text += " " + line.strip()
                # Reset the empty line counter
                empty_line_count = 0
            # Check if we are in the text section and the line is empty
            elif is_text and not line.strip():
                # Increment the empty line counter
                empty_line_count += 1
                # If two consecutive lines are empty, stop reading text
                if empty_line_count == 2:
                    is_text = False
        # Extract the numeric part of the url_id from the file name
        url_id_numeric = int(file.split('blackassign')[1].split('.')[0])
        text_data.append({'URL_ID': url_id_numeric, 'Title': title, 'Text': text})

# Create a DataFrame from the extracted data
extracted_df = pd.DataFrame(text_data)

# Sort the DataFrame based on the numeric part of 'URL_ID'
extracted_df.sort_values('URL_ID', inplace=True)

# Convert the numeric part back to the original 'URL_ID' format with zero padding
extracted_df['URL_ID'] = 'blackassign' + extracted_df['URL_ID'].astype(str).str.zfill(4)

# Reset the index of the DataFrame
extracted_df.reset_index(drop=True, inplace=True)

print(extracted_df)

extracted_df.head()

# Load the stop words from files in the "StopWords" folder
stop_words = set()
stop_words_folder = '/content/drive/MyDrive/BlackCoffer Data Engineer Assignment/Provided files/StopWords folder/StopWords/'
for file_path in glob.glob(stop_words_folder + '*.txt'):
    with open(file_path, 'r', encoding='latin-1') as f:
        words = f.read().splitlines()
        # Convert words to lowercase before adding them to the set
        stop_words.update([word.lower() for word in words])

# Function for the clean_text
def clean_text(text):
    # Convert text to lowercase
    text = text.lower()
    # Replace full stops with a special character
    text = text.replace('.', ' | ')
    # Remove extra spaces
    text = ' '.join(text.split())
    # Remove alphanumeric and numeric values
    text = re.sub(r'\w*\d\w*', '', text)
    # Remove special characters except the sentence boundary marker
    text = re.sub(r'[^a-zA-Z\s\|]', '', text)
    # Remove stopwords
    words = text.split()
    cleaned_words = [word for word in words if word.strip() not in stop_words]
    cleaned_text = ' '.join(cleaned_words)
    return cleaned_text
# Apply text cleaning to the 'Text' column in the DataFrame
extracted_df['Cleaned_Text'] = extracted_df['Text'].apply(clean_text)

# path to the stop words folder
stop_words_folder = r"/content/drive/MyDrive/BlackCoffer Data Engineer Assignment/Provided files/StopWords folder/StopWords/"

# Get the list of files in the stop words folder
stop_words_files = glob.glob(stop_words_folder + '*.txt')

# print the number of files and their names
print(f"Number of stop words files: {len(stop_words_files)}")
for file_path in stop_words_files:
    print(file_path)

# Read the positive and negative words lists
positive_words = set()
with open(r"/content/drive/MyDrive/BlackCoffer Data Engineer Assignment/Provided files/MasterDictionary folder/MasterDictionary/positive-words.txt", 'r', encoding='latin-1') as f:
    positive_words = set(f.read().splitlines())

negative_words = set()
with open(r"/content/drive/MyDrive/BlackCoffer Data Engineer Assignment/Provided files/MasterDictionary folder/MasterDictionary/negative-words.txt", 'r', encoding='latin-1') as f:
    negative_words = set(f.read().splitlines())

# Assuming 'stop_words' is a set of stop words already defined
# Remove the stop words from positive and negative words lists
positive_words = positive_words - stop_words
negative_words = negative_words - stop_words

# Create a dictionary of positive and negative words
positive_dict = {word: 1 for word in positive_words}
negative_dict = {word: -1 for word in negative_words}

# Merge positive and negative dictionaries into a single sentiment dictionary
sentiment_dict = {**positive_dict, **negative_dict}

# Function to calculate the positive and negative scores
def calculate_scores(text):
    # Tokenize the text
    tokens = word_tokenize(text.lower())
    # Calculate the positive and negative scores
    positive_score = sum(1 for word in tokens if word in positive_words)
    negative_score = sum(1 for word in tokens if word in negative_words)
    return positive_score,negative_score
# Apply the function to the 'Text' column in the DataFrame
extracted_df['POSITIVE_SCORE'],extracted_df['NEGATIVE_SCORE'] = zip(*extracted_df['Cleaned_Text'].apply(calculate_scores))

# Function to calculate polarity score
def calculate_polarity_score(positive_score,negative_score):
    return (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)
# Calculate Polarity Score for each row in the DataFrame
extracted_df['POLARITY_SCORE'] = extracted_df.apply(lambda row: calculate_polarity_score(row['POSITIVE_SCORE'], row['NEGATIVE_SCORE']), axis=1)

# Function to calculate subjectivity score
def calculate_subjectivity_score(positive_score, negative_score, cleaned_text):
    total_words = len(cleaned_text.split())
    return (positive_score + negative_score) / (total_words + 0.000001)
# Calculate Subjectivity Score for each row in the DataFrame
extracted_df['SUBJECTIVITY_SCORE'] = extracted_df.apply(lambda row: calculate_subjectivity_score(row['POSITIVE_SCORE'], row['NEGATIVE_SCORE'], row['Cleaned_Text']), axis=1)

# Function to calculate average sentence length
def calculate_average_sentence_length(text):
    sentences = sent_tokenize(text)
    Total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)
    Total_sentences = len(sentences)
    return Total_words / Total_sentences

# Function to calcualte percentage of complex words
def calculate_percentage_complex_words(text):
    words = word_tokenize(text.lower())
    # Assuming words with more than 2 syllables are complex
    complex_words = [word for word in words if textstat.syllable_count(word) > 2]
    return len(complex_words) / len(words) if len(words) > 0 else 0

# Function to calculate Gunning Fox index
def calculate_gunning_fox_index(text):
    avg_sent_len = calculate_average_sentence_length(text)
    percentage_complex_words = calculate_percentage_complex_words(text)
    fog_index = 0.4 * (avg_sent_len + percentage_complex_words)
    return fog_index
# Apply the functions to the 'Text' column in the DataFrame
extracted_df['AVG_SENTENCE_LENGTH'] = extracted_df['Text'].apply(calculate_average_sentence_length)
extracted_df['PERCENTAGE_OF_COMPLEX_WORDS'] = extracted_df['Text'].apply(calculate_percentage_complex_words)
extracted_df['FOG_INDEX'] = extracted_df['Text'].apply(calculate_gunning_fox_index)

from nltk.tokenize import sent_tokenize, word_tokenize

def calculate_avg_words_per_sentence(text):
    # Tokenize the text into sentences
    sentences = sent_tokenize(text)
    # Calculate the total number of words
    total_words = len(word_tokenize(text))
    # Calculate the total number of sentences
    total_sentences = len(sentences)
    # Calculate the average number of words per sentence
    avg_words_per_sentence = total_words / total_sentences
    return avg_words_per_sentence
# Apply the function to the 'Text' column in the DataFrame
extracted_df['AVG_NUMBER_OF_WORD_PER_SENTENCE'] = extracted_df['Text'].apply(calculate_avg_words_per_sentence)

# Function to calculate complex word count
def complex_word_count(text):
    # Tokenize the text into words
    tokens = word_tokenize(text)
    # Count the complex words
    complex_words = [token for token in tokens if textstat.syllable_count(token)>2]
    return len(complex_words)
# Apply the function to the 'Text' column in the DataFrame
extracted_df['COMPLEX_WORD_COUNT'] = extracted_df['Text'].apply(complex_word_count)

# Function to remove stop words and count cleaned tokens
def word_count(text):
    # Tokenize the text into words
    tokens = word_tokenize(text)
    # Count the word count
    cleaned_tokens = [token for token in tokens if token not in stopwords.words('english')]
    # Count the cleaned tokens
    word_count = len(cleaned_tokens)
    return word_count
# Apply the function to the 'Text' column in the DataFrame
extracted_df['WORD_COUNT'] = extracted_df['Text'].apply(word_count)

# Function to syllable count per word
def count_syllables(word):
    vowels = "aeiouy"
    count = 0
    if word[0] in vowels:
        count += 1
    for index in range(1, len(word)):
        if word[index] in vowels and word[index - 1] not in vowels:
            count += 1
    if word.endswith("e"):
        count -= 1
    if word.endswith("le") and len(word) > 2 and word[-3] not in vowels:
        count += 1
    if word.endswith("es") or word.endswith("ed"):
        count -= 1
    if count == 0:
        count += 1
    return count
# Function to count syllables in each token
def count_syllables_in_tokens(tokens):
    # Tokenize the text into words
    Tokens = word_tokenize(text)
    # Count syllables in each token
    syllable_counts = [count_syllables(token) for token in tokens]
    return syllable_counts
# Apply the function to the 'Tokens' column in the DataFrame
extracted_df['SYLLABLE_PER_WORD'] = extracted_df['Text'].apply(count_syllables_in_tokens)

# Function to calculate personal pronouns count
def calculate_personal_pronouns_count(text):
    # Define the regex pattern
    pattern = r'\b(?:I|we|my|ours|us)\b'
    # Find all matches in the text
    matches = re.findall(pattern, text, flags=re.IGNORECASE)
    # Return the count of matches
    return len(matches)
# Apply the function to the 'Text' column in the DataFrame
extracted_df['PERSONAL_PRONOUNS'] = extracted_df['Text'].apply(calculate_personal_pronouns_count)

# Function to calculate the average word length
def calculate_average_word_length(tokens):
    # Tokenize the text into words
    tokens = word_tokenize(text)
    # Calculate the total number of characters in each word
    total_characters = sum(len(token) for token in tokens)
    # Calculate total number of words
    total_words= len(tokens)
    # Calculate the average word length
    average_word_length = total_characters / total_words if total_words else 0
    return average_word_length
# Apply the function to the 'Tokens' column in the DataFrame
extracted_df['AVG_WORD_LENGTH'] = extracted_df['Text'].apply(calculate_average_word_length)

# Assuming 'extracted_df' is your existing DataFrame with 'URL_ID', 'Title', and 'Text'
# Get the maximum URL_ID number from the DataFrame
max_url_id_num = extracted_df['URL_ID'].str.extract('(\d+)').astype(int).max()[0]
# Create a complete list of URL_IDs that should be present based on the maximum number found
complete_url_ids = ['blackassign' + str(i).zfill(4) for i in range(1, max_url_id_num + 1)]
# Convert the 'URL_ID' column to a list for comparison
existing_url_ids = extracted_df['URL_ID'].tolist()
# Identify missing URL_IDs
missing_url_ids = set(complete_url_ids) - set(existing_url_ids)
# Create rows with NaN values for the missing URL_IDs
missing_data = [{'URL_ID': url_id, 'Title': np.nan, 'Text': np.nan} for url_id in missing_url_ids]
# Convert the missing data to a DataFrame
missing_df = pd.DataFrame(missing_data)
# Append the missing data to the original DataFrame
complete_df = pd.concat([extracted_df, missing_df])
# Sort the DataFrame based on 'URL_ID'
complete_df.sort_values('URL_ID', inplace=True)
# Reset the index of the DataFrame
complete_df.reset_index(drop=True, inplace=True)

print(complete_df)

complete_df=complete_df.drop(columns=['Title','Text','Cleaned_Text'])

complete_df.loc[35]

complete_df.loc[48]

print(input_file)
input_data = pd.DataFrame(input_file)

output_data_structure = pd.merge(input_data,complete_df,on='URL_ID')

output_data_structure.head()

!pip install openpyxl

# Save the DataFrame to an Excel file
output_data_structure.to_excel('/content/drive/MyDrive/BlackCoffer Data Engineer Assignment/output_data.xlsx', index=False)